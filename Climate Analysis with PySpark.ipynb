{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setting up your Spark instance"
      ],
      "metadata": {
        "id": "gB7kgb5w4fpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Spark and Set up Environment"
      ],
      "metadata": {
        "id": "iBr3PSfy4ns-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHDp6AzS3u72",
        "outputId": "342fadc1-6822-4b89-ba3d-d3d3cbbc57d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.0\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317122 sha256=28cb3e3f96e4609fa4b1e84301533810294340d2147079c7e236cefae12cbc6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "# Run below commands\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #Install java\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz ## Install Apache Spark\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark==3.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "# Define Java and Spark home path in Google Colab\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "rbJFxiWZ43EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# from pyspark import SparkConf, SparkContext\n",
        "from datetime import datetime, date, timedelta\n",
        "from dateutil import relativedelta\n",
        "from pyspark.sql import SQLContext, Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import to_timestamp, to_date\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import collect_list, collect_set, concat, first, array_distinct, col, size, expr\n",
        "import random"
      ],
      "metadata": {
        "id": "0dzL99tj44ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"End-to-End Example\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "LW04MoKp4874"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End-to-End Example"
      ],
      "metadata": {
        "id": "K2GQk0gl6UG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read the data\n",
        "flightData2015 = spark.read.option('inferSchema','true').option('header','true').csv('/content/drive/My Drive/flight-data/2015-summary.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED-alop-6Zl6",
        "outputId": "f29dd7c3-5289-4ae2-b64c-10a1b0472ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOHhxOAM9bTZ",
        "outputId": "12fbc7be-a5dc-4c34-b789-99149cbe74c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.sort('count').explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oktR-cyP_CRW",
        "outputId": "0f84aed2-5c94-4104-f78d-426ed120357d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=33]\n",
            "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/drive/My Drive/flight-data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spar.sql.shuffle.partitions\",\"5\")\n",
        "flightData2015.sort(\"count\").take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLyAk6RN_qVM",
        "outputId": "ec59f053-d903-4941-e553-cde8e0ef163a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
              " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.createOrReplaceGlobalTempView(\"flight_data_2015\")"
      ],
      "metadata": {
        "id": "-TEl05VxY95-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqlWay = spark.sql(\"\"\"\n",
        "select DEST_COUNTRY_NAME, count(1)\n",
        "from global_temp.flight_data_2015\n",
        "group by DEST_COUNTRY_NAME\"\"\")\n",
        "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
        "sqlWay.explain()\n",
        "dataFrameWay.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2T7Tus9RBXi",
        "outputId": "23783e41-4393-424e-b43a-bc7315331dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 200), ENSURE_REQUIREMENTS, [plan_id=55]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/drive/My Drive/flight-data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 200), ENSURE_REQUIREMENTS, [plan_id=68]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/drive/My Drive/flight-data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select max(count) from global_temp.flight_data_2015\").take(1)"
      ],
      "metadata": {
        "id": "RDuotaOXTuq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d67fca-bf69-475e-e94f-5dd46393108e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "flightData2015.select(max(\"count\")).take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McS5MzAEX9Bg",
        "outputId": "cae8dcbb-6a7e-40cb-e51d-f945951635e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxSql = spark.sql(\"\"\"\n",
        "    select DEST_COUNTRY_NAME, sum(count) as destination_total\n",
        "    from global_temp.flight_data_2015\n",
        "    group by DEST_COUNTRY_NAME\n",
        "    order by sum(count) DESC\n",
        "    limit 5\n",
        "\"\"\")\n",
        "maxSql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYGl7gcrYORc",
        "outputId": "d83c4d85-a916-4537-8803-5d9563214c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\",\"destination_total\").sort(desc(\"destination_total\")).limit(5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP6V_6PyZqCJ",
        "outputId": "065118f3-790b-402c-edcc-b12e6177a3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Climate Change"
      ],
      "metadata": {
        "id": "INRFu7Qpal1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Climate Analysis\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "Jc7uTmrjf_IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The first dataset: temperature"
      ],
      "metadata": {
        "id": "QRcvVvzVb_ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your file is accessible at a given path\n",
        "file_path = \"/content/drive/My Drive/UC Davis - Spring/GlobalLandTemperatures_GlobalLandTemperaturesByCountry.csv\"\n",
        "\n",
        "# Load the data into a DataFrame\n",
        "temperature = spark.read.option('inferSchema','true').option('header','true').csv(file_path)"
      ],
      "metadata": {
        "id": "CkaoZciggJz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4pCOXcohHgS",
        "outputId": "b9ca4361-55aa-4285-ea3d-62cfe23e48fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(dt=datetime.date(1743, 11, 1), AverageTemperature=4.3839999999999995, AverageTemperatureUncertainty=2.294, Country='Åland'),\n",
              " Row(dt=datetime.date(1743, 12, 1), AverageTemperature=None, AverageTemperatureUncertainty=None, Country='Åland'),\n",
              " Row(dt=datetime.date(1744, 1, 1), AverageTemperature=None, AverageTemperatureUncertainty=None, Country='Åland'),\n",
              " Row(dt=datetime.date(1744, 2, 1), AverageTemperature=None, AverageTemperatureUncertainty=None, Country='Åland'),\n",
              " Row(dt=datetime.date(1744, 3, 1), AverageTemperature=None, AverageTemperatureUncertainty=None, Country='Åland')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temperature.createOrReplaceGlobalTempView(\"temperature\")"
      ],
      "metadata": {
        "id": "yJ46pUE8g2iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxSql = spark.sql(\"\"\"\n",
        "    select Country, dt, year(dt) as year, AverageTemperature\n",
        "    from global_temp.temperature\n",
        "    order by AverageTemperature desc\n",
        "    limit 1\n",
        "\"\"\")\n",
        "maxSql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA9pbbvCgsHe",
        "outputId": "04f297e1-bfc5-4303-8708-8b7d8a44bfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+------------------+\n",
            "|Country|        dt|year|AverageTemperature|\n",
            "+-------+----------+----+------------------+\n",
            "| Kuwait|2012-07-01|2012| 38.84200000000001|\n",
            "+-------+----------+----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest average temperature was observed for Kuwait during 2012."
      ],
      "metadata": {
        "id": "2tYkUt39k02U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diffSql = spark.sql(\"\"\"\n",
        "    select country, (max(AverageTemperature)-min(AverageTemperature)) as temp_diff\n",
        "    from global_temp.temperature\n",
        "    group by country\n",
        "    order by temp_diff desc\n",
        "    limit 10\n",
        "\"\"\")\n",
        "diffSql.show()"
      ],
      "metadata": {
        "id": "1A4VvDHloL7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "744463f2-ae9b-4b76-b0d4-0c013fcf6a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------------+\n",
            "|     country|         temp_diff|\n",
            "+------------+------------------+\n",
            "|  Kazakhstan|            49.163|\n",
            "|    Mongolia|48.157999999999994|\n",
            "|      Russia|             47.47|\n",
            "|      Canada|            43.532|\n",
            "|  Uzbekistan|            42.698|\n",
            "|Turkmenistan|            40.579|\n",
            "|     Finland|            40.332|\n",
            "|     Belarus|            39.338|\n",
            "|     Ukraine|            39.021|\n",
            "|     Estonia|38.882999999999996|\n",
            "+------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The second dataset: CO2"
      ],
      "metadata": {
        "id": "7TYDdNVgd3vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the second dataset\n",
        "file_path = \"/content/drive/My Drive/UC Davis - Spring/CO2 emissions per capita per country.csv\"\n",
        "\n",
        "# Load the data into a DataFrame\n",
        "co2 = spark.read.option('inferSchema','true').option('header','true').csv(file_path)"
      ],
      "metadata": {
        "id": "P0sY9OtI3xS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Transform the structure of co2\n",
        "years = [str(y) for y in range(1960, 2015)]\n",
        "cols = \", \".join([\"'{}', {}\".format(year, \"`{}`\".format(year)) for year in years])\n",
        "stack_expr = \"stack({}, {}) as (year, CO2_Per_Capita)\".format(len(years), cols)\n",
        "co2_df_transformed = co2.select(\"`Country Name`\", expr(stack_expr)).where(\"CO2_Per_Capita is not null\")\n",
        "co2_df_transformed.createOrReplaceTempView(\"co2_transformed\")"
      ],
      "metadata": {
        "id": "vQNMGQVr_TJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "from pyspark.sql.functions import year\n",
        "\n",
        "# Add year column to the temperature dataset\n",
        "temp_df = temperature.withColumn(\"year\", year(\"dt\"))\n",
        "temp_df.createOrReplaceTempView(\"avg_temperature\")"
      ],
      "metadata": {
        "id": "WrgZgv1nBEQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge two lists without aggregation"
      ],
      "metadata": {
        "id": "EdFND7fI8Wsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "SELECT\n",
        "    t.Country,\n",
        "    t.year,\n",
        "    t.AverageTemperature,\n",
        "    c.CO2_Per_Capita\n",
        "FROM avg_temperature t\n",
        "JOIN co2_transformed c\n",
        "ON t.Country = c.`Country Name` AND t.year = c.year\n",
        "WHERE t.year BETWEEN 1960 AND 2014\n",
        "\"\"\"\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eFYTFPL8anC",
        "outputId": "eb328a21-df59-4685-c86c-d70b5e513dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+------------------+--------------+\n",
            "|    Country|year|AverageTemperature|CO2_Per_Capita|\n",
            "+-----------+----+------------------+--------------+\n",
            "|Afghanistan|1960|             2.262|   0.046059897|\n",
            "|Afghanistan|1960| 7.007999999999999|   0.046059897|\n",
            "|Afghanistan|1960| 5.832000000000002|   0.046059897|\n",
            "|Afghanistan|1960|12.312000000000001|   0.046059897|\n",
            "|Afghanistan|1960|            18.853|   0.046059897|\n",
            "|Afghanistan|1960|            25.436|   0.046059897|\n",
            "|Afghanistan|1960|              26.6|   0.046059897|\n",
            "|Afghanistan|1960|            25.749|   0.046059897|\n",
            "|Afghanistan|1960|            20.537|   0.046059897|\n",
            "|Afghanistan|1960|             14.27|   0.046059897|\n",
            "|Afghanistan|1960|             7.155|   0.046059897|\n",
            "|Afghanistan|1960|             1.811|   0.046059897|\n",
            "|Afghanistan|1961|             1.482|   0.053604304|\n",
            "|Afghanistan|1961|              1.59|   0.053604304|\n",
            "|Afghanistan|1961| 9.260000000000002|   0.053604304|\n",
            "|Afghanistan|1961|            12.818|   0.053604304|\n",
            "|Afghanistan|1961|            21.318|   0.053604304|\n",
            "|Afghanistan|1961|            24.656|   0.053604304|\n",
            "|Afghanistan|1961|27.363000000000003|   0.053604304|\n",
            "|Afghanistan|1961|            25.263|   0.053604304|\n",
            "+-----------+----+------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge two lists with aggregation: Temperature change in a year"
      ],
      "metadata": {
        "id": "8WXvmbOq8bHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculate temperature change for each year in each country\n",
        "temp_range_df = temp_df.groupBy(\"Country\", \"year\")\\\n",
        "                       .agg(F.max(\"AverageTemperature\").alias(\"MaxTemperature\"),\n",
        "                            F.min(\"AverageTemperature\").alias(\"MinTemperature\"))\n",
        "\n",
        "\n",
        "# Create the dataframe\n",
        "temp_range_df = temp_range_df.withColumn(\"TemperatureRange\",\n",
        "                                         F.col(\"MaxTemperature\") - F.col(\"MinTemperature\"))\n",
        "temp_range_df.createOrReplaceTempView(\"temperature_range\")"
      ],
      "metadata": {
        "id": "P6OogHSzwo7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_range_df.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rX7tj3BwvR_",
        "outputId": "38034667-ba9b-4154-e640-bba96d05b72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Country='Albania', year=1821, MaxTemperature=20.861, MinTemperature=2.539, TemperatureRange=18.322),\n",
              " Row(Country='Albania', year=1943, MaxTemperature=23.89, MinTemperature=1.446, TemperatureRange=22.444),\n",
              " Row(Country='Albania', year=1960, MaxTemperature=22.531, MinTemperature=4.127, TemperatureRange=18.404)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge two datasets by country\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "    t.Country,\n",
        "    t.year,\n",
        "    t.TemperatureRange,\n",
        "    c.CO2_Per_Capita\n",
        "FROM temperature_range t\n",
        "JOIN co2_transformed c\n",
        "ON t.Country = c.`Country Name` AND t.year = c.year\n",
        "WHERE t.year BETWEEN 1960 AND 2014\n",
        "order by Country, year\n",
        "\"\"\"\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRklg6Hx_HXW",
        "outputId": "e6ea82f1-b435-49e4-878c-c6ccac8248c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+------------------+--------------+\n",
            "|    Country|year|  TemperatureRange|CO2_Per_Capita|\n",
            "+-----------+----+------------------+--------------+\n",
            "|Afghanistan|1960|            24.789|   0.046059897|\n",
            "|Afghanistan|1961|25.881000000000004|   0.053604304|\n",
            "|Afghanistan|1962|            26.345|   0.073764791|\n",
            "|Afghanistan|1963|            24.746|   0.074232685|\n",
            "|Afghanistan|1964|30.723999999999997|   0.086292452|\n",
            "|Afghanistan|1965|            24.679|   0.101467397|\n",
            "|Afghanistan|1966|            23.475|   0.107636955|\n",
            "|Afghanistan|1967|27.567999999999998|   0.123734289|\n",
            "|Afghanistan|1968|            25.204|    0.11549774|\n",
            "|Afghanistan|1969|28.424999999999997|    0.08682346|\n",
            "|Afghanistan|1970|            25.525|   0.150290627|\n",
            "|Afghanistan|1971|            27.144|   0.166042044|\n",
            "|Afghanistan|1972|            28.685|    0.13076385|\n",
            "|Afghanistan|1973|29.387999999999998|   0.136279785|\n",
            "|Afghanistan|1974|27.743999999999996|   0.155649444|\n",
            "|Afghanistan|1975|             26.83|   0.168928649|\n",
            "|Afghanistan|1976|25.473999999999997|   0.154787206|\n",
            "|Afghanistan|1977|            30.005|   0.182963616|\n",
            "|Afghanistan|1978|            26.663|   0.163159571|\n",
            "|Afghanistan|1979|25.778000000000002|   0.168376671|\n",
            "+-----------+----+------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation between CO2 and temperature\n",
        "correlation = result_df.stat.corr(\"TemperatureRange\", \"CO2_Per_Capita\")\n",
        "print(\"The Pearson correlation coefficient between average temperature and CO2 per capita is:\", correlation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53z09inzDWfO",
        "outputId": "8f78f346-e67d-4d3c-d8db-f93696e2b95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Pearson correlation coefficient between average temperature and CO2 per capita is: 0.36907906344988894\n"
          ]
        }
      ]
    }
  ]
}